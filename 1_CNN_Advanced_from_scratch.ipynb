{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8040c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636211f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269b3b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sandbox\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed7c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f3afc",
   "metadata": {},
   "source": [
    "# torch에서 몇몇 모델들을 기본적으로 제공해주긴 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d914b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import googlenet\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6959126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a49a2a",
   "metadata": {},
   "source": [
    "## 그래도 한번씩 모델을 직접 짜보도록 하죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0db27",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76dbff",
   "metadata": {},
   "source": [
    "<img src=\"img/AlexNet.png\" width=\"600px\" height=\"400px\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c60564",
   "metadata": {},
   "source": [
    "<img src=\"img/alexnet(2).png\" width=\"600px\" height=\"800px\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a903b4",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.4em;line-height:1.5em'>Input Size가 224\\*224\\*3이 아닌 227\\*227\\*3으로 변경되었습니다. 관련 자료는 아래 link를 참조하세요</span>\n",
    "\n",
    "https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389dce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, dropout=0.5):\n",
    "        super(MyAlexNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "                                   nn.ReLU(inplace=True), \n",
    "                                   nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(96,256,kernel_size=5, padding=2), \n",
    "                                   nn.ReLU(inplace=True), \n",
    "                                   nn.MaxPool2d(kernel_size=3,stride=2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(256, 384, kernel_size=3, padding=1), \n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(384, 384, kernel_size=3, padding=1), \n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(384, 256, kernel_size=3, padding=1), \n",
    "                                   nn.ReLU(inplace=True), \n",
    "                                   nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.fc = nn.Sequential(nn.Dropout(p=dropout),\n",
    "                                nn.Linear(6*6*256, 4096), \n",
    "                                nn.ReLU(inplace=True), \n",
    "                                nn.Dropout(p=dropout), \n",
    "                                nn.Linear(4096,4096), \n",
    "                                nn.ReLU(inplace=True), \n",
    "                                nn.Linear(4096, num_classes))\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(submodule.weight)\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0)\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0) # 해당 submodule의 bias는 0으로 초기화\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        out = F.softmax(x, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62e3e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyAlexNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyAlexNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0621b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "              ReLU-2           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
      "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
      "              ReLU-5          [-1, 256, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-10          [-1, 384, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "          Dropout-14                 [-1, 9216]               0\n",
      "           Linear-15                 [-1, 4096]      37,752,832\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "           Linear-20                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 62,378,344\n",
      "Trainable params: 62,378,344\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 11.09\n",
      "Params size (MB): 237.95\n",
      "Estimated Total Size (MB): 249.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 227, 227), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90482be7",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1175e93",
   "metadata": {},
   "source": [
    "<img src=\"img/vgg16.png\" width=\"600px\" height=\"800px\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3be59",
   "metadata": {},
   "source": [
    "<img src=\"img/VGG16(2).png\" width=\"600px\" height=\"400px\"></img><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2e85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(config):\n",
    "    layers = []\n",
    "    in_planes = 3\n",
    "    for value in config:\n",
    "        if value == \"M\":\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            layers.append(nn.Conv2d(in_planes, value, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_planes = value\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_configs = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfb4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = make_layer(vgg16_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aecdd9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU()\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU()\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU()\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU()\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU()\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU()\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU()\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU()\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU()\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU()\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU()\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a01260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=1000, dropout=0.5, initialize_weight = False):\n",
    "        super(MyVGG16, self).__init__()\n",
    "        self.convs = make_layer(vgg16_configs)\n",
    "        self.fc = nn.Sequential(nn.Linear(512 * 7 * 7, 4096),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.5),\n",
    "                                nn.Linear(4096, 4096),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.5),\n",
    "                                nn.Linear(4096, num_classes))\n",
    "        if initialize_weight:\n",
    "            self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        out = F.softmax(x, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce10cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyVGG16(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU()\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU()\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU()\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU()\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU()\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyVGG16().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1344881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 227, 227]           1,792\n",
      "              ReLU-2         [-1, 64, 227, 227]               0\n",
      "            Conv2d-3         [-1, 64, 227, 227]          36,928\n",
      "              ReLU-4         [-1, 64, 227, 227]               0\n",
      "         MaxPool2d-5         [-1, 64, 113, 113]               0\n",
      "            Conv2d-6        [-1, 128, 113, 113]          73,856\n",
      "              ReLU-7        [-1, 128, 113, 113]               0\n",
      "            Conv2d-8        [-1, 128, 113, 113]         147,584\n",
      "              ReLU-9        [-1, 128, 113, 113]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "           Linear-32                 [-1, 4096]     102,764,544\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "           Linear-35                 [-1, 4096]      16,781,312\n",
      "             ReLU-36                 [-1, 4096]               0\n",
      "          Dropout-37                 [-1, 4096]               0\n",
      "           Linear-38                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 222.22\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 750.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 227, 227), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063bb70",
   "metadata": {},
   "source": [
    "## GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61014941",
   "metadata": {},
   "source": [
    "<img src=\"img/GoogleNet.png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcba57f",
   "metadata": {},
   "source": [
    "<img src=\"img/GoogleNet(2).png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf5a40",
   "metadata": {},
   "source": [
    "(참고: https://devlee247.com/papers/2022-06-20-googlenet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c475ea",
   "metadata": {},
   "source": [
    "### Step1. Inception Module 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd2a8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BaseConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ReLU(self.conv(x))\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3_red, ch3x3, ch5x5_red, ch5x5, pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
    "        \n",
    "        self.conv3x3 = nn.Sequential(BaseConv2d(in_channels, ch3x3_red, kernel_size=1),\n",
    "                                     BaseConv2d(ch3x3_red, ch3x3, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv5x5 = nn.Sequential(BaseConv2d(in_channels, ch5x5_red, kernel_size=1),\n",
    "                                     BaseConv2d(ch5x5_red, ch5x5, kernel_size=5, padding=2))\n",
    "        \n",
    "        self.pool = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                                  BaseConv2d(in_channels, pool, kernel_size=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.pool(x)\n",
    "        \n",
    "        # x1,x2,x3,x4는 각각 (batch_size, n_channel, height, width)로 된 4차원 tensor\n",
    "        # channel concat --> 1차원 방향으로 concatenate\n",
    "        return torch.cat([x1, x2, x3, x4], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91a332",
   "metadata": {},
   "source": [
    "### step2. Auxiliary Module 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb139c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxModule(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AuxModule, self).__init__()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.conv1 = BaseConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4*4*128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.7),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64bc9f",
   "metadata": {},
   "source": [
    "### Step3. GoogLeNet 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76436afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGoogleNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(MyGoogleNet, self).__init__()\n",
    "        self.is_training=True\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3,64,kernel_size=7, stride=2, padding=3), \n",
    "                                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                                   nn.LocalResponseNorm(2))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(BaseConv2d(64, 64, kernel_size=1),\n",
    "                                   BaseConv2d(64, 192, kernel_size=3, padding=1),\n",
    "                                   nn.LocalResponseNorm(2),\n",
    "                                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        \n",
    "        self.inception_3a = InceptionModule(in_channels=192, \n",
    "                                            ch1x1=64, \n",
    "                                            ch3x3_red=96, \n",
    "                                            ch3x3=128, \n",
    "                                            ch5x5_red=16, \n",
    "                                            ch5x5=32, \n",
    "                                            pool=32)\n",
    "        self.inception_3b = InceptionModule(in_channels=256, \n",
    "                                            ch1x1=128, \n",
    "                                            ch3x3_red=128, \n",
    "                                            ch3x3=192, \n",
    "                                            ch5x5_red=32, \n",
    "                                            ch5x5=96, \n",
    "                                            pool=64)\n",
    "        self.maxpool_3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception_4a = InceptionModule(in_channels=480, \n",
    "                                            ch1x1=192, \n",
    "                                            ch3x3_red=96, \n",
    "                                            ch3x3=208, \n",
    "                                            ch5x5_red=16, \n",
    "                                            ch5x5=48, \n",
    "                                            pool=64)\n",
    "        self.aux1 = AuxModule(512, num_classes)\n",
    "\n",
    "        self.inception_4b = InceptionModule(in_channels=512, \n",
    "                                            ch1x1=160, \n",
    "                                            ch3x3_red=112, \n",
    "                                            ch3x3=224, \n",
    "                                            ch5x5_red=24, \n",
    "                                            ch5x5=64, \n",
    "                                            pool=64)\n",
    "        self.inception_4c = InceptionModule(in_channels=512, \n",
    "                                            ch1x1=128, \n",
    "                                            ch3x3_red=128, \n",
    "                                            ch3x3=256,\n",
    "                                            ch5x5_red=24,\n",
    "                                            ch5x5=64,\n",
    "                                            pool=64)\n",
    "        self.inception_4d = InceptionModule(in_channels=512,\n",
    "                                            ch1x1=112,\n",
    "                                            ch3x3_red=144,\n",
    "                                            ch3x3=288,\n",
    "                                            ch5x5_red=32,\n",
    "                                            ch5x5=64,\n",
    "                                            pool=64)\n",
    "        self.aux2 = AuxModule(528, num_classes)\n",
    "\n",
    "        self.inception_4e = InceptionModule(in_channels=528,\n",
    "                                            ch1x1=256,\n",
    "                                            ch3x3_red=160,\n",
    "                                            ch3x3=320,\n",
    "                                            ch5x5_red=32,\n",
    "                                            ch5x5=128,\n",
    "                                            pool=128)\n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception_5a = InceptionModule(in_channels=832, \n",
    "                                            ch1x1=256, \n",
    "                                            ch3x3_red=160,\n",
    "                                            ch3x3=320,\n",
    "                                            ch5x5_red=32,\n",
    "                                            ch5x5=128,\n",
    "                                            pool=128)\n",
    "        self.inception_5b = InceptionModule(in_channels=832,\n",
    "                                            ch1x1=384,\n",
    "                                            ch3x3_red=192,\n",
    "                                            ch3x3=384,\n",
    "                                            ch5x5_red=48,\n",
    "                                            ch5x5=128,\n",
    "                                            pool=128)\n",
    "\n",
    "        # AdaptiveAvgPool explanation\n",
    "        # https://stackoverflow.com/questions/58692476/what-is-adaptive-average-pooling-and-how-does-it-work\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.inception_3a(x)\n",
    "        x = self.inception_3b(x)\n",
    "        x = self.maxpool_3(x)\n",
    "\n",
    "        x = self.inception_4a(x)\n",
    "        if self.is_training:\n",
    "            out1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception_4b(x)\n",
    "        x = self.inception_4c(x)\n",
    "        x = self.inception_4d(x)\n",
    "        if self.is_training:\n",
    "            out2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception_4e(x)\n",
    "        x = self.maxpool_4(x)\n",
    "\n",
    "        x = self.inception_5a(x)\n",
    "        x = self.inception_5b(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)                \n",
    "        if self.is_training:\n",
    "            return [x, out1, out2]\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d9297e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGoogleNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (2): LocalResponseNorm(2, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): BaseConv2d(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ReLU): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BaseConv2d(\n",
      "      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (ReLU): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): LocalResponseNorm(2, alpha=0.0001, beta=0.75, k=1.0)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (inception_3a): InceptionModule(\n",
      "    (conv1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inception_3b): InceptionModule(\n",
      "    (conv1x1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (maxpool_3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception_4a): InceptionModule(\n",
      "    (conv1x1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aux1): AuxModule(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "    (conv1): BaseConv2d(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ReLU): ReLU(inplace=True)\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout2d(p=0.7, inplace=False)\n",
      "      (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (inception_4b): InceptionModule(\n",
      "    (conv1x1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inception_4c): InceptionModule(\n",
      "    (conv1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inception_4d): InceptionModule(\n",
      "    (conv1x1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aux2): AuxModule(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "    (conv1): BaseConv2d(\n",
      "      (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ReLU): ReLU(inplace=True)\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout2d(p=0.7, inplace=False)\n",
      "      (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (inception_4e): InceptionModule(\n",
      "    (conv1x1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (maxpool_4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception_5a): InceptionModule(\n",
      "    (conv1x1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inception_5b): InceptionModule(\n",
      "    (conv1x1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3x3): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv5x5): Sequential(\n",
      "      (0): BaseConv2d(\n",
      "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): BaseConv2d(\n",
      "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (ReLU): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyGoogleNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59989f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 114, 114]           9,472\n",
      "         MaxPool2d-2           [-1, 64, 57, 57]               0\n",
      " LocalResponseNorm-3           [-1, 64, 57, 57]               0\n",
      "            Conv2d-4           [-1, 64, 57, 57]           4,160\n",
      "              ReLU-5           [-1, 64, 57, 57]               0\n",
      "        BaseConv2d-6           [-1, 64, 57, 57]               0\n",
      "            Conv2d-7          [-1, 192, 57, 57]         110,784\n",
      "              ReLU-8          [-1, 192, 57, 57]               0\n",
      "        BaseConv2d-9          [-1, 192, 57, 57]               0\n",
      "LocalResponseNorm-10          [-1, 192, 57, 57]               0\n",
      "        MaxPool2d-11          [-1, 192, 29, 29]               0\n",
      "           Conv2d-12           [-1, 64, 29, 29]          12,352\n",
      "           Conv2d-13           [-1, 96, 29, 29]          18,528\n",
      "             ReLU-14           [-1, 96, 29, 29]               0\n",
      "       BaseConv2d-15           [-1, 96, 29, 29]               0\n",
      "           Conv2d-16          [-1, 128, 29, 29]         110,720\n",
      "             ReLU-17          [-1, 128, 29, 29]               0\n",
      "       BaseConv2d-18          [-1, 128, 29, 29]               0\n",
      "           Conv2d-19           [-1, 16, 29, 29]           3,088\n",
      "             ReLU-20           [-1, 16, 29, 29]               0\n",
      "       BaseConv2d-21           [-1, 16, 29, 29]               0\n",
      "           Conv2d-22           [-1, 32, 29, 29]          12,832\n",
      "             ReLU-23           [-1, 32, 29, 29]               0\n",
      "       BaseConv2d-24           [-1, 32, 29, 29]               0\n",
      "        MaxPool2d-25          [-1, 192, 29, 29]               0\n",
      "           Conv2d-26           [-1, 32, 29, 29]           6,176\n",
      "             ReLU-27           [-1, 32, 29, 29]               0\n",
      "       BaseConv2d-28           [-1, 32, 29, 29]               0\n",
      "  InceptionModule-29          [-1, 256, 29, 29]               0\n",
      "           Conv2d-30          [-1, 128, 29, 29]          32,896\n",
      "           Conv2d-31          [-1, 128, 29, 29]          32,896\n",
      "             ReLU-32          [-1, 128, 29, 29]               0\n",
      "       BaseConv2d-33          [-1, 128, 29, 29]               0\n",
      "           Conv2d-34          [-1, 192, 29, 29]         221,376\n",
      "             ReLU-35          [-1, 192, 29, 29]               0\n",
      "       BaseConv2d-36          [-1, 192, 29, 29]               0\n",
      "           Conv2d-37           [-1, 32, 29, 29]           8,224\n",
      "             ReLU-38           [-1, 32, 29, 29]               0\n",
      "       BaseConv2d-39           [-1, 32, 29, 29]               0\n",
      "           Conv2d-40           [-1, 96, 29, 29]          76,896\n",
      "             ReLU-41           [-1, 96, 29, 29]               0\n",
      "       BaseConv2d-42           [-1, 96, 29, 29]               0\n",
      "        MaxPool2d-43          [-1, 256, 29, 29]               0\n",
      "           Conv2d-44           [-1, 64, 29, 29]          16,448\n",
      "             ReLU-45           [-1, 64, 29, 29]               0\n",
      "       BaseConv2d-46           [-1, 64, 29, 29]               0\n",
      "  InceptionModule-47          [-1, 480, 29, 29]               0\n",
      "        MaxPool2d-48          [-1, 480, 15, 15]               0\n",
      "           Conv2d-49          [-1, 192, 15, 15]          92,352\n",
      "           Conv2d-50           [-1, 96, 15, 15]          46,176\n",
      "             ReLU-51           [-1, 96, 15, 15]               0\n",
      "       BaseConv2d-52           [-1, 96, 15, 15]               0\n",
      "           Conv2d-53          [-1, 208, 15, 15]         179,920\n",
      "             ReLU-54          [-1, 208, 15, 15]               0\n",
      "       BaseConv2d-55          [-1, 208, 15, 15]               0\n",
      "           Conv2d-56           [-1, 16, 15, 15]           7,696\n",
      "             ReLU-57           [-1, 16, 15, 15]               0\n",
      "       BaseConv2d-58           [-1, 16, 15, 15]               0\n",
      "           Conv2d-59           [-1, 48, 15, 15]          19,248\n",
      "             ReLU-60           [-1, 48, 15, 15]               0\n",
      "       BaseConv2d-61           [-1, 48, 15, 15]               0\n",
      "        MaxPool2d-62          [-1, 480, 15, 15]               0\n",
      "           Conv2d-63           [-1, 64, 15, 15]          30,784\n",
      "             ReLU-64           [-1, 64, 15, 15]               0\n",
      "       BaseConv2d-65           [-1, 64, 15, 15]               0\n",
      "  InceptionModule-66          [-1, 512, 15, 15]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 4, 4]               0\n",
      "           Conv2d-68            [-1, 128, 4, 4]          65,664\n",
      "             ReLU-69            [-1, 128, 4, 4]               0\n",
      "       BaseConv2d-70            [-1, 128, 4, 4]               0\n",
      "           Linear-71                 [-1, 1024]       2,098,176\n",
      "             ReLU-72                 [-1, 1024]               0\n",
      "        Dropout2d-73                 [-1, 1024]               0\n",
      "           Linear-74                 [-1, 1000]       1,025,000\n",
      "        AuxModule-75                 [-1, 1000]               0\n",
      "           Conv2d-76          [-1, 160, 15, 15]          82,080\n",
      "           Conv2d-77          [-1, 112, 15, 15]          57,456\n",
      "             ReLU-78          [-1, 112, 15, 15]               0\n",
      "       BaseConv2d-79          [-1, 112, 15, 15]               0\n",
      "           Conv2d-80          [-1, 224, 15, 15]         226,016\n",
      "             ReLU-81          [-1, 224, 15, 15]               0\n",
      "       BaseConv2d-82          [-1, 224, 15, 15]               0\n",
      "           Conv2d-83           [-1, 24, 15, 15]          12,312\n",
      "             ReLU-84           [-1, 24, 15, 15]               0\n",
      "       BaseConv2d-85           [-1, 24, 15, 15]               0\n",
      "           Conv2d-86           [-1, 64, 15, 15]          38,464\n",
      "             ReLU-87           [-1, 64, 15, 15]               0\n",
      "       BaseConv2d-88           [-1, 64, 15, 15]               0\n",
      "        MaxPool2d-89          [-1, 512, 15, 15]               0\n",
      "           Conv2d-90           [-1, 64, 15, 15]          32,832\n",
      "             ReLU-91           [-1, 64, 15, 15]               0\n",
      "       BaseConv2d-92           [-1, 64, 15, 15]               0\n",
      "  InceptionModule-93          [-1, 512, 15, 15]               0\n",
      "           Conv2d-94          [-1, 128, 15, 15]          65,664\n",
      "           Conv2d-95          [-1, 128, 15, 15]          65,664\n",
      "             ReLU-96          [-1, 128, 15, 15]               0\n",
      "       BaseConv2d-97          [-1, 128, 15, 15]               0\n",
      "           Conv2d-98          [-1, 256, 15, 15]         295,168\n",
      "             ReLU-99          [-1, 256, 15, 15]               0\n",
      "      BaseConv2d-100          [-1, 256, 15, 15]               0\n",
      "          Conv2d-101           [-1, 24, 15, 15]          12,312\n",
      "            ReLU-102           [-1, 24, 15, 15]               0\n",
      "      BaseConv2d-103           [-1, 24, 15, 15]               0\n",
      "          Conv2d-104           [-1, 64, 15, 15]          38,464\n",
      "            ReLU-105           [-1, 64, 15, 15]               0\n",
      "      BaseConv2d-106           [-1, 64, 15, 15]               0\n",
      "       MaxPool2d-107          [-1, 512, 15, 15]               0\n",
      "          Conv2d-108           [-1, 64, 15, 15]          32,832\n",
      "            ReLU-109           [-1, 64, 15, 15]               0\n",
      "      BaseConv2d-110           [-1, 64, 15, 15]               0\n",
      " InceptionModule-111          [-1, 512, 15, 15]               0\n",
      "          Conv2d-112          [-1, 112, 15, 15]          57,456\n",
      "          Conv2d-113          [-1, 144, 15, 15]          73,872\n",
      "            ReLU-114          [-1, 144, 15, 15]               0\n",
      "      BaseConv2d-115          [-1, 144, 15, 15]               0\n",
      "          Conv2d-116          [-1, 288, 15, 15]         373,536\n",
      "            ReLU-117          [-1, 288, 15, 15]               0\n",
      "      BaseConv2d-118          [-1, 288, 15, 15]               0\n",
      "          Conv2d-119           [-1, 32, 15, 15]          16,416\n",
      "            ReLU-120           [-1, 32, 15, 15]               0\n",
      "      BaseConv2d-121           [-1, 32, 15, 15]               0\n",
      "          Conv2d-122           [-1, 64, 15, 15]          51,264\n",
      "            ReLU-123           [-1, 64, 15, 15]               0\n",
      "      BaseConv2d-124           [-1, 64, 15, 15]               0\n",
      "       MaxPool2d-125          [-1, 512, 15, 15]               0\n",
      "          Conv2d-126           [-1, 64, 15, 15]          32,832\n",
      "            ReLU-127           [-1, 64, 15, 15]               0\n",
      "      BaseConv2d-128           [-1, 64, 15, 15]               0\n",
      " InceptionModule-129          [-1, 528, 15, 15]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 528, 4, 4]               0\n",
      "          Conv2d-131            [-1, 128, 4, 4]          67,712\n",
      "            ReLU-132            [-1, 128, 4, 4]               0\n",
      "      BaseConv2d-133            [-1, 128, 4, 4]               0\n",
      "          Linear-134                 [-1, 1024]       2,098,176\n",
      "            ReLU-135                 [-1, 1024]               0\n",
      "       Dropout2d-136                 [-1, 1024]               0\n",
      "          Linear-137                 [-1, 1000]       1,025,000\n",
      "       AuxModule-138                 [-1, 1000]               0\n",
      "          Conv2d-139          [-1, 256, 15, 15]         135,424\n",
      "          Conv2d-140          [-1, 160, 15, 15]          84,640\n",
      "            ReLU-141          [-1, 160, 15, 15]               0\n",
      "      BaseConv2d-142          [-1, 160, 15, 15]               0\n",
      "          Conv2d-143          [-1, 320, 15, 15]         461,120\n",
      "            ReLU-144          [-1, 320, 15, 15]               0\n",
      "      BaseConv2d-145          [-1, 320, 15, 15]               0\n",
      "          Conv2d-146           [-1, 32, 15, 15]          16,928\n",
      "            ReLU-147           [-1, 32, 15, 15]               0\n",
      "      BaseConv2d-148           [-1, 32, 15, 15]               0\n",
      "          Conv2d-149          [-1, 128, 15, 15]         102,528\n",
      "            ReLU-150          [-1, 128, 15, 15]               0\n",
      "      BaseConv2d-151          [-1, 128, 15, 15]               0\n",
      "       MaxPool2d-152          [-1, 528, 15, 15]               0\n",
      "          Conv2d-153          [-1, 128, 15, 15]          67,712\n",
      "            ReLU-154          [-1, 128, 15, 15]               0\n",
      "      BaseConv2d-155          [-1, 128, 15, 15]               0\n",
      " InceptionModule-156          [-1, 832, 15, 15]               0\n",
      "       MaxPool2d-157            [-1, 832, 8, 8]               0\n",
      "          Conv2d-158            [-1, 256, 8, 8]         213,248\n",
      "          Conv2d-159            [-1, 160, 8, 8]         133,280\n",
      "            ReLU-160            [-1, 160, 8, 8]               0\n",
      "      BaseConv2d-161            [-1, 160, 8, 8]               0\n",
      "          Conv2d-162            [-1, 320, 8, 8]         461,120\n",
      "            ReLU-163            [-1, 320, 8, 8]               0\n",
      "      BaseConv2d-164            [-1, 320, 8, 8]               0\n",
      "          Conv2d-165             [-1, 32, 8, 8]          26,656\n",
      "            ReLU-166             [-1, 32, 8, 8]               0\n",
      "      BaseConv2d-167             [-1, 32, 8, 8]               0\n",
      "          Conv2d-168            [-1, 128, 8, 8]         102,528\n",
      "            ReLU-169            [-1, 128, 8, 8]               0\n",
      "      BaseConv2d-170            [-1, 128, 8, 8]               0\n",
      "       MaxPool2d-171            [-1, 832, 8, 8]               0\n",
      "          Conv2d-172            [-1, 128, 8, 8]         106,624\n",
      "            ReLU-173            [-1, 128, 8, 8]               0\n",
      "      BaseConv2d-174            [-1, 128, 8, 8]               0\n",
      " InceptionModule-175            [-1, 832, 8, 8]               0\n",
      "          Conv2d-176            [-1, 384, 8, 8]         319,872\n",
      "          Conv2d-177            [-1, 192, 8, 8]         159,936\n",
      "            ReLU-178            [-1, 192, 8, 8]               0\n",
      "      BaseConv2d-179            [-1, 192, 8, 8]               0\n",
      "          Conv2d-180            [-1, 384, 8, 8]         663,936\n",
      "            ReLU-181            [-1, 384, 8, 8]               0\n",
      "      BaseConv2d-182            [-1, 384, 8, 8]               0\n",
      "          Conv2d-183             [-1, 48, 8, 8]          39,984\n",
      "            ReLU-184             [-1, 48, 8, 8]               0\n",
      "      BaseConv2d-185             [-1, 48, 8, 8]               0\n",
      "          Conv2d-186            [-1, 128, 8, 8]         153,728\n",
      "            ReLU-187            [-1, 128, 8, 8]               0\n",
      "      BaseConv2d-188            [-1, 128, 8, 8]               0\n",
      "       MaxPool2d-189            [-1, 832, 8, 8]               0\n",
      "          Conv2d-190            [-1, 128, 8, 8]         106,624\n",
      "            ReLU-191            [-1, 128, 8, 8]               0\n",
      "      BaseConv2d-192            [-1, 128, 8, 8]               0\n",
      " InceptionModule-193           [-1, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-194           [-1, 1024, 1, 1]               0\n",
      "       Dropout2d-195                 [-1, 1024]               0\n",
      "          Linear-196                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 13,378,280\n",
      "Trainable params: 13,378,280\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 90.38\n",
      "Params size (MB): 51.03\n",
      "Estimated Total Size (MB): 142.00\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANDBOX\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 227, 227), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb3cd2",
   "metadata": {},
   "source": [
    "## ResNet (18-layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c18f53",
   "metadata": {},
   "source": [
    "<img src=\"img/resnet.png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cac84",
   "metadata": {},
   "source": [
    "<img src=\"img/resnet(2).png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd0a69",
   "metadata": {},
   "source": [
    "### 18-layer resnet구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "189c265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_ch, out_ch, stride=1, groups=1, dilation=1):\n",
    "    r\"\"\"\n",
    "    3x3 convolution with padding\n",
    "    - in_planes: in_channels\n",
    "    - out_channels: out_channels\n",
    "    - bias=False: BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정.\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7292e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50d15a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet18(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(MyResNet18, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.zero_init_residual = zero_init_residual\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        \n",
    "        # each element in the tuple indicates if we should replace\n",
    "        # the 2x2 stride with a dilated convolution instead\n",
    "        if replace_stride_with_dilation is None:    \n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        \n",
    "        \n",
    "        # 구조 정의\n",
    "        self.conv1   = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1     = norm_layer(self.inplanes)\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.block2 = self._make_layer(block, 64, layers[0])\n",
    "        self.block3 = self._make_layer(block, 128, layers[1], stride=2, \n",
    "                                       dilate = replace_stride_with_dilation[0])\n",
    "        self.block4 = self._make_layer(block, 256, layers[2], stride=2, \n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.block5 = self._make_layer(block, 512, layers[3], stride=2, \n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        if self.zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "                \n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "#     def _forward_impl(self, x):\n",
    "#         # See note [TorchScript super()]\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        result = self.fc(x)\n",
    "        return result\n",
    "        \n",
    "#         return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4ef4be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv1x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyResNet18(block\u001b[38;5;241m=\u001b[39mBasicBlock, layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mMyResNet18.__init__\u001b[1;34m(self, block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m64\u001b[39m, layers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m128\u001b[39m, layers[\u001b[38;5;241m1\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m     34\u001b[0m                                dilate \u001b[38;5;241m=\u001b[39m replace_stride_with_dilation[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m256\u001b[39m, layers[\u001b[38;5;241m2\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m     36\u001b[0m                                dilate\u001b[38;5;241m=\u001b[39mreplace_stride_with_dilation[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_layer(block, \u001b[38;5;241m512\u001b[39m, layers[\u001b[38;5;241m3\u001b[39m], stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m     38\u001b[0m                                dilate\u001b[38;5;241m=\u001b[39mreplace_stride_with_dilation[\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[1;32mIn[27], line 65\u001b[0m, in \u001b[0;36mMyResNet18._make_layer\u001b[1;34m(self, block, planes, blocks, stride, dilate)\u001b[0m\n\u001b[0;32m     62\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplanes \u001b[38;5;241m!=\u001b[39m planes \u001b[38;5;241m*\u001b[39m block\u001b[38;5;241m.\u001b[39mexpansion:\n\u001b[0;32m     64\u001b[0m     downsample \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m---> 65\u001b[0m         conv1x1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplanes, planes \u001b[38;5;241m*\u001b[39m block\u001b[38;5;241m.\u001b[39mexpansion, stride),\n\u001b[0;32m     66\u001b[0m         norm_layer(planes \u001b[38;5;241m*\u001b[39m block\u001b[38;5;241m.\u001b[39mexpansion),\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     69\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m layers\u001b[38;5;241m.\u001b[39mappend(block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplanes, planes, stride, downsample, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m     71\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_width, previous_dilation, norm_layer))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv1x1' is not defined"
     ]
    }
   ],
   "source": [
    "model = MyResNet18(block=BasicBlock, layers=[2,2,2,2]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35afb1",
   "metadata": {},
   "source": [
    "## DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013891e",
   "metadata": {},
   "source": [
    "<img src=\"img/DenseNet.png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e17a4",
   "metadata": {},
   "source": [
    "<img src=\"img/DenseNet(2).png\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dec2d1",
   "metadata": {},
   "source": [
    "### (1) BottleNeck 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "560472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        inner_channels = 4 * growth_rate\n",
    "\n",
    "        self.residual = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
    "                                      nn.BatchNorm2d(inner_channels),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False))\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.shortcut(x), self.residual(x)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300e75a",
   "metadata": {},
   "source": [
    "### (2) Transition layer구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2b5aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.down_sample = nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "                                         nn.AvgPool2d(2, stride=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58201e",
   "metadata": {},
   "source": [
    "### (3) DenseNet구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec5ff71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseNet121(nn.Module):\n",
    "    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=10, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.growth_rate = growth_rate\n",
    "        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, inner_channels, 7, stride=2, padding=3),\n",
    "                                   nn.MaxPool2d(3, 2, padding=1))\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        ## Dense block, transition layer 1~3까지 생성\n",
    "        for i in range(len(nblocks)-1):\n",
    "            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n",
    "            inner_channels += growth_rate * nblocks[i]\n",
    "            out_channels = int(reduction * inner_channels) #downsampling out_channel\n",
    "            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n",
    "            inner_channels = out_channels \n",
    "        \n",
    "        ## Dense block 4생성\n",
    "        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n",
    "        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n",
    "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
    "        self.features.add_module('relu', nn.ReLU())\n",
    "        \n",
    "        ## Global Avg를 AdaptiveAvgPool2d를 사용하여 수행\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(inner_channels, num_classes)\n",
    "\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def _make_dense_block(self, nblock, inner_channels):\n",
    "        dense_block = nn.Sequential()\n",
    "        for i in range(nblock):\n",
    "            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n",
    "            inner_channels += self.growth_rate\n",
    "        return dense_block\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f060022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDenseNet121(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (dense_block_0): Sequential(\n",
      "      (bottle_neck_layer_0): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_1): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_2): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_3): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_4): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_5): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (transition_layer_0): Transition(\n",
      "      (down_sample): Sequential(\n",
      "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "    )\n",
      "    (dense_block_1): Sequential(\n",
      "      (bottle_neck_layer_0): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_1): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_2): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_3): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_4): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_5): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_6): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_7): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_8): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_9): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_10): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_11): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (transition_layer_1): Transition(\n",
      "      (down_sample): Sequential(\n",
      "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "    )\n",
      "    (dense_block_2): Sequential(\n",
      "      (bottle_neck_layer_0): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_1): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_2): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_3): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_4): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_5): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_6): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_7): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_8): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_9): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_10): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_11): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_12): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_13): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_14): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_15): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_16): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_17): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(300, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_18): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(312, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_19): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(324, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_20): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(336, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_21): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(348, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_22): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(360, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_23): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(372, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (transition_layer_2): Transition(\n",
      "      (down_sample): Sequential(\n",
      "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "    )\n",
      "    (dense_block_3): Sequential(\n",
      "      (bottle_neck_layer_0): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_1): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_2): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_3): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_4): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (bottle_neck_layer_5): BottleNeck(\n",
      "        (residual): Sequential(\n",
      "          (0): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (bn): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (linear): Linear(in_features=264, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyDenseNet121([6,12,24,6]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d72cf698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 112, 112]           3,552\n",
      "         MaxPool2d-2           [-1, 24, 56, 56]               0\n",
      "       BatchNorm2d-3           [-1, 24, 56, 56]              48\n",
      "              ReLU-4           [-1, 24, 56, 56]               0\n",
      "            Conv2d-5           [-1, 48, 56, 56]           1,152\n",
      "       BatchNorm2d-6           [-1, 48, 56, 56]              96\n",
      "              ReLU-7           [-1, 48, 56, 56]               0\n",
      "            Conv2d-8           [-1, 12, 56, 56]           5,184\n",
      "        BottleNeck-9           [-1, 36, 56, 56]               0\n",
      "      BatchNorm2d-10           [-1, 36, 56, 56]              72\n",
      "             ReLU-11           [-1, 36, 56, 56]               0\n",
      "           Conv2d-12           [-1, 48, 56, 56]           1,728\n",
      "      BatchNorm2d-13           [-1, 48, 56, 56]              96\n",
      "             ReLU-14           [-1, 48, 56, 56]               0\n",
      "           Conv2d-15           [-1, 12, 56, 56]           5,184\n",
      "       BottleNeck-16           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-17           [-1, 48, 56, 56]              96\n",
      "             ReLU-18           [-1, 48, 56, 56]               0\n",
      "           Conv2d-19           [-1, 48, 56, 56]           2,304\n",
      "      BatchNorm2d-20           [-1, 48, 56, 56]              96\n",
      "             ReLU-21           [-1, 48, 56, 56]               0\n",
      "           Conv2d-22           [-1, 12, 56, 56]           5,184\n",
      "       BottleNeck-23           [-1, 60, 56, 56]               0\n",
      "      BatchNorm2d-24           [-1, 60, 56, 56]             120\n",
      "             ReLU-25           [-1, 60, 56, 56]               0\n",
      "           Conv2d-26           [-1, 48, 56, 56]           2,880\n",
      "      BatchNorm2d-27           [-1, 48, 56, 56]              96\n",
      "             ReLU-28           [-1, 48, 56, 56]               0\n",
      "           Conv2d-29           [-1, 12, 56, 56]           5,184\n",
      "       BottleNeck-30           [-1, 72, 56, 56]               0\n",
      "      BatchNorm2d-31           [-1, 72, 56, 56]             144\n",
      "             ReLU-32           [-1, 72, 56, 56]               0\n",
      "           Conv2d-33           [-1, 48, 56, 56]           3,456\n",
      "      BatchNorm2d-34           [-1, 48, 56, 56]              96\n",
      "             ReLU-35           [-1, 48, 56, 56]               0\n",
      "           Conv2d-36           [-1, 12, 56, 56]           5,184\n",
      "       BottleNeck-37           [-1, 84, 56, 56]               0\n",
      "      BatchNorm2d-38           [-1, 84, 56, 56]             168\n",
      "             ReLU-39           [-1, 84, 56, 56]               0\n",
      "           Conv2d-40           [-1, 48, 56, 56]           4,032\n",
      "      BatchNorm2d-41           [-1, 48, 56, 56]              96\n",
      "             ReLU-42           [-1, 48, 56, 56]               0\n",
      "           Conv2d-43           [-1, 12, 56, 56]           5,184\n",
      "       BottleNeck-44           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-45           [-1, 96, 56, 56]             192\n",
      "             ReLU-46           [-1, 96, 56, 56]               0\n",
      "           Conv2d-47           [-1, 48, 56, 56]           4,608\n",
      "        AvgPool2d-48           [-1, 48, 28, 28]               0\n",
      "       Transition-49           [-1, 48, 28, 28]               0\n",
      "      BatchNorm2d-50           [-1, 48, 28, 28]              96\n",
      "             ReLU-51           [-1, 48, 28, 28]               0\n",
      "           Conv2d-52           [-1, 48, 28, 28]           2,304\n",
      "      BatchNorm2d-53           [-1, 48, 28, 28]              96\n",
      "             ReLU-54           [-1, 48, 28, 28]               0\n",
      "           Conv2d-55           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-56           [-1, 60, 28, 28]               0\n",
      "      BatchNorm2d-57           [-1, 60, 28, 28]             120\n",
      "             ReLU-58           [-1, 60, 28, 28]               0\n",
      "           Conv2d-59           [-1, 48, 28, 28]           2,880\n",
      "      BatchNorm2d-60           [-1, 48, 28, 28]              96\n",
      "             ReLU-61           [-1, 48, 28, 28]               0\n",
      "           Conv2d-62           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-63           [-1, 72, 28, 28]               0\n",
      "      BatchNorm2d-64           [-1, 72, 28, 28]             144\n",
      "             ReLU-65           [-1, 72, 28, 28]               0\n",
      "           Conv2d-66           [-1, 48, 28, 28]           3,456\n",
      "      BatchNorm2d-67           [-1, 48, 28, 28]              96\n",
      "             ReLU-68           [-1, 48, 28, 28]               0\n",
      "           Conv2d-69           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-70           [-1, 84, 28, 28]               0\n",
      "      BatchNorm2d-71           [-1, 84, 28, 28]             168\n",
      "             ReLU-72           [-1, 84, 28, 28]               0\n",
      "           Conv2d-73           [-1, 48, 28, 28]           4,032\n",
      "      BatchNorm2d-74           [-1, 48, 28, 28]              96\n",
      "             ReLU-75           [-1, 48, 28, 28]               0\n",
      "           Conv2d-76           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-77           [-1, 96, 28, 28]               0\n",
      "      BatchNorm2d-78           [-1, 96, 28, 28]             192\n",
      "             ReLU-79           [-1, 96, 28, 28]               0\n",
      "           Conv2d-80           [-1, 48, 28, 28]           4,608\n",
      "      BatchNorm2d-81           [-1, 48, 28, 28]              96\n",
      "             ReLU-82           [-1, 48, 28, 28]               0\n",
      "           Conv2d-83           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-84          [-1, 108, 28, 28]               0\n",
      "      BatchNorm2d-85          [-1, 108, 28, 28]             216\n",
      "             ReLU-86          [-1, 108, 28, 28]               0\n",
      "           Conv2d-87           [-1, 48, 28, 28]           5,184\n",
      "      BatchNorm2d-88           [-1, 48, 28, 28]              96\n",
      "             ReLU-89           [-1, 48, 28, 28]               0\n",
      "           Conv2d-90           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-91          [-1, 120, 28, 28]               0\n",
      "      BatchNorm2d-92          [-1, 120, 28, 28]             240\n",
      "             ReLU-93          [-1, 120, 28, 28]               0\n",
      "           Conv2d-94           [-1, 48, 28, 28]           5,760\n",
      "      BatchNorm2d-95           [-1, 48, 28, 28]              96\n",
      "             ReLU-96           [-1, 48, 28, 28]               0\n",
      "           Conv2d-97           [-1, 12, 28, 28]           5,184\n",
      "       BottleNeck-98          [-1, 132, 28, 28]               0\n",
      "      BatchNorm2d-99          [-1, 132, 28, 28]             264\n",
      "            ReLU-100          [-1, 132, 28, 28]               0\n",
      "          Conv2d-101           [-1, 48, 28, 28]           6,336\n",
      "     BatchNorm2d-102           [-1, 48, 28, 28]              96\n",
      "            ReLU-103           [-1, 48, 28, 28]               0\n",
      "          Conv2d-104           [-1, 12, 28, 28]           5,184\n",
      "      BottleNeck-105          [-1, 144, 28, 28]               0\n",
      "     BatchNorm2d-106          [-1, 144, 28, 28]             288\n",
      "            ReLU-107          [-1, 144, 28, 28]               0\n",
      "          Conv2d-108           [-1, 48, 28, 28]           6,912\n",
      "     BatchNorm2d-109           [-1, 48, 28, 28]              96\n",
      "            ReLU-110           [-1, 48, 28, 28]               0\n",
      "          Conv2d-111           [-1, 12, 28, 28]           5,184\n",
      "      BottleNeck-112          [-1, 156, 28, 28]               0\n",
      "     BatchNorm2d-113          [-1, 156, 28, 28]             312\n",
      "            ReLU-114          [-1, 156, 28, 28]               0\n",
      "          Conv2d-115           [-1, 48, 28, 28]           7,488\n",
      "     BatchNorm2d-116           [-1, 48, 28, 28]              96\n",
      "            ReLU-117           [-1, 48, 28, 28]               0\n",
      "          Conv2d-118           [-1, 12, 28, 28]           5,184\n",
      "      BottleNeck-119          [-1, 168, 28, 28]               0\n",
      "     BatchNorm2d-120          [-1, 168, 28, 28]             336\n",
      "            ReLU-121          [-1, 168, 28, 28]               0\n",
      "          Conv2d-122           [-1, 48, 28, 28]           8,064\n",
      "     BatchNorm2d-123           [-1, 48, 28, 28]              96\n",
      "            ReLU-124           [-1, 48, 28, 28]               0\n",
      "          Conv2d-125           [-1, 12, 28, 28]           5,184\n",
      "      BottleNeck-126          [-1, 180, 28, 28]               0\n",
      "     BatchNorm2d-127          [-1, 180, 28, 28]             360\n",
      "            ReLU-128          [-1, 180, 28, 28]               0\n",
      "          Conv2d-129           [-1, 48, 28, 28]           8,640\n",
      "     BatchNorm2d-130           [-1, 48, 28, 28]              96\n",
      "            ReLU-131           [-1, 48, 28, 28]               0\n",
      "          Conv2d-132           [-1, 12, 28, 28]           5,184\n",
      "      BottleNeck-133          [-1, 192, 28, 28]               0\n",
      "     BatchNorm2d-134          [-1, 192, 28, 28]             384\n",
      "            ReLU-135          [-1, 192, 28, 28]               0\n",
      "          Conv2d-136           [-1, 96, 28, 28]          18,432\n",
      "       AvgPool2d-137           [-1, 96, 14, 14]               0\n",
      "      Transition-138           [-1, 96, 14, 14]               0\n",
      "     BatchNorm2d-139           [-1, 96, 14, 14]             192\n",
      "            ReLU-140           [-1, 96, 14, 14]               0\n",
      "          Conv2d-141           [-1, 48, 14, 14]           4,608\n",
      "     BatchNorm2d-142           [-1, 48, 14, 14]              96\n",
      "            ReLU-143           [-1, 48, 14, 14]               0\n",
      "          Conv2d-144           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-145          [-1, 108, 14, 14]               0\n",
      "     BatchNorm2d-146          [-1, 108, 14, 14]             216\n",
      "            ReLU-147          [-1, 108, 14, 14]               0\n",
      "          Conv2d-148           [-1, 48, 14, 14]           5,184\n",
      "     BatchNorm2d-149           [-1, 48, 14, 14]              96\n",
      "            ReLU-150           [-1, 48, 14, 14]               0\n",
      "          Conv2d-151           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-152          [-1, 120, 14, 14]               0\n",
      "     BatchNorm2d-153          [-1, 120, 14, 14]             240\n",
      "            ReLU-154          [-1, 120, 14, 14]               0\n",
      "          Conv2d-155           [-1, 48, 14, 14]           5,760\n",
      "     BatchNorm2d-156           [-1, 48, 14, 14]              96\n",
      "            ReLU-157           [-1, 48, 14, 14]               0\n",
      "          Conv2d-158           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-159          [-1, 132, 14, 14]               0\n",
      "     BatchNorm2d-160          [-1, 132, 14, 14]             264\n",
      "            ReLU-161          [-1, 132, 14, 14]               0\n",
      "          Conv2d-162           [-1, 48, 14, 14]           6,336\n",
      "     BatchNorm2d-163           [-1, 48, 14, 14]              96\n",
      "            ReLU-164           [-1, 48, 14, 14]               0\n",
      "          Conv2d-165           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-166          [-1, 144, 14, 14]               0\n",
      "     BatchNorm2d-167          [-1, 144, 14, 14]             288\n",
      "            ReLU-168          [-1, 144, 14, 14]               0\n",
      "          Conv2d-169           [-1, 48, 14, 14]           6,912\n",
      "     BatchNorm2d-170           [-1, 48, 14, 14]              96\n",
      "            ReLU-171           [-1, 48, 14, 14]               0\n",
      "          Conv2d-172           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-173          [-1, 156, 14, 14]               0\n",
      "     BatchNorm2d-174          [-1, 156, 14, 14]             312\n",
      "            ReLU-175          [-1, 156, 14, 14]               0\n",
      "          Conv2d-176           [-1, 48, 14, 14]           7,488\n",
      "     BatchNorm2d-177           [-1, 48, 14, 14]              96\n",
      "            ReLU-178           [-1, 48, 14, 14]               0\n",
      "          Conv2d-179           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-180          [-1, 168, 14, 14]               0\n",
      "     BatchNorm2d-181          [-1, 168, 14, 14]             336\n",
      "            ReLU-182          [-1, 168, 14, 14]               0\n",
      "          Conv2d-183           [-1, 48, 14, 14]           8,064\n",
      "     BatchNorm2d-184           [-1, 48, 14, 14]              96\n",
      "            ReLU-185           [-1, 48, 14, 14]               0\n",
      "          Conv2d-186           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-187          [-1, 180, 14, 14]               0\n",
      "     BatchNorm2d-188          [-1, 180, 14, 14]             360\n",
      "            ReLU-189          [-1, 180, 14, 14]               0\n",
      "          Conv2d-190           [-1, 48, 14, 14]           8,640\n",
      "     BatchNorm2d-191           [-1, 48, 14, 14]              96\n",
      "            ReLU-192           [-1, 48, 14, 14]               0\n",
      "          Conv2d-193           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-194          [-1, 192, 14, 14]               0\n",
      "     BatchNorm2d-195          [-1, 192, 14, 14]             384\n",
      "            ReLU-196          [-1, 192, 14, 14]               0\n",
      "          Conv2d-197           [-1, 48, 14, 14]           9,216\n",
      "     BatchNorm2d-198           [-1, 48, 14, 14]              96\n",
      "            ReLU-199           [-1, 48, 14, 14]               0\n",
      "          Conv2d-200           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-201          [-1, 204, 14, 14]               0\n",
      "     BatchNorm2d-202          [-1, 204, 14, 14]             408\n",
      "            ReLU-203          [-1, 204, 14, 14]               0\n",
      "          Conv2d-204           [-1, 48, 14, 14]           9,792\n",
      "     BatchNorm2d-205           [-1, 48, 14, 14]              96\n",
      "            ReLU-206           [-1, 48, 14, 14]               0\n",
      "          Conv2d-207           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-208          [-1, 216, 14, 14]               0\n",
      "     BatchNorm2d-209          [-1, 216, 14, 14]             432\n",
      "            ReLU-210          [-1, 216, 14, 14]               0\n",
      "          Conv2d-211           [-1, 48, 14, 14]          10,368\n",
      "     BatchNorm2d-212           [-1, 48, 14, 14]              96\n",
      "            ReLU-213           [-1, 48, 14, 14]               0\n",
      "          Conv2d-214           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-215          [-1, 228, 14, 14]               0\n",
      "     BatchNorm2d-216          [-1, 228, 14, 14]             456\n",
      "            ReLU-217          [-1, 228, 14, 14]               0\n",
      "          Conv2d-218           [-1, 48, 14, 14]          10,944\n",
      "     BatchNorm2d-219           [-1, 48, 14, 14]              96\n",
      "            ReLU-220           [-1, 48, 14, 14]               0\n",
      "          Conv2d-221           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-222          [-1, 240, 14, 14]               0\n",
      "     BatchNorm2d-223          [-1, 240, 14, 14]             480\n",
      "            ReLU-224          [-1, 240, 14, 14]               0\n",
      "          Conv2d-225           [-1, 48, 14, 14]          11,520\n",
      "     BatchNorm2d-226           [-1, 48, 14, 14]              96\n",
      "            ReLU-227           [-1, 48, 14, 14]               0\n",
      "          Conv2d-228           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-229          [-1, 252, 14, 14]               0\n",
      "     BatchNorm2d-230          [-1, 252, 14, 14]             504\n",
      "            ReLU-231          [-1, 252, 14, 14]               0\n",
      "          Conv2d-232           [-1, 48, 14, 14]          12,096\n",
      "     BatchNorm2d-233           [-1, 48, 14, 14]              96\n",
      "            ReLU-234           [-1, 48, 14, 14]               0\n",
      "          Conv2d-235           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-236          [-1, 264, 14, 14]               0\n",
      "     BatchNorm2d-237          [-1, 264, 14, 14]             528\n",
      "            ReLU-238          [-1, 264, 14, 14]               0\n",
      "          Conv2d-239           [-1, 48, 14, 14]          12,672\n",
      "     BatchNorm2d-240           [-1, 48, 14, 14]              96\n",
      "            ReLU-241           [-1, 48, 14, 14]               0\n",
      "          Conv2d-242           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-243          [-1, 276, 14, 14]               0\n",
      "     BatchNorm2d-244          [-1, 276, 14, 14]             552\n",
      "            ReLU-245          [-1, 276, 14, 14]               0\n",
      "          Conv2d-246           [-1, 48, 14, 14]          13,248\n",
      "     BatchNorm2d-247           [-1, 48, 14, 14]              96\n",
      "            ReLU-248           [-1, 48, 14, 14]               0\n",
      "          Conv2d-249           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-250          [-1, 288, 14, 14]               0\n",
      "     BatchNorm2d-251          [-1, 288, 14, 14]             576\n",
      "            ReLU-252          [-1, 288, 14, 14]               0\n",
      "          Conv2d-253           [-1, 48, 14, 14]          13,824\n",
      "     BatchNorm2d-254           [-1, 48, 14, 14]              96\n",
      "            ReLU-255           [-1, 48, 14, 14]               0\n",
      "          Conv2d-256           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-257          [-1, 300, 14, 14]               0\n",
      "     BatchNorm2d-258          [-1, 300, 14, 14]             600\n",
      "            ReLU-259          [-1, 300, 14, 14]               0\n",
      "          Conv2d-260           [-1, 48, 14, 14]          14,400\n",
      "     BatchNorm2d-261           [-1, 48, 14, 14]              96\n",
      "            ReLU-262           [-1, 48, 14, 14]               0\n",
      "          Conv2d-263           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-264          [-1, 312, 14, 14]               0\n",
      "     BatchNorm2d-265          [-1, 312, 14, 14]             624\n",
      "            ReLU-266          [-1, 312, 14, 14]               0\n",
      "          Conv2d-267           [-1, 48, 14, 14]          14,976\n",
      "     BatchNorm2d-268           [-1, 48, 14, 14]              96\n",
      "            ReLU-269           [-1, 48, 14, 14]               0\n",
      "          Conv2d-270           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-271          [-1, 324, 14, 14]               0\n",
      "     BatchNorm2d-272          [-1, 324, 14, 14]             648\n",
      "            ReLU-273          [-1, 324, 14, 14]               0\n",
      "          Conv2d-274           [-1, 48, 14, 14]          15,552\n",
      "     BatchNorm2d-275           [-1, 48, 14, 14]              96\n",
      "            ReLU-276           [-1, 48, 14, 14]               0\n",
      "          Conv2d-277           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-278          [-1, 336, 14, 14]               0\n",
      "     BatchNorm2d-279          [-1, 336, 14, 14]             672\n",
      "            ReLU-280          [-1, 336, 14, 14]               0\n",
      "          Conv2d-281           [-1, 48, 14, 14]          16,128\n",
      "     BatchNorm2d-282           [-1, 48, 14, 14]              96\n",
      "            ReLU-283           [-1, 48, 14, 14]               0\n",
      "          Conv2d-284           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-285          [-1, 348, 14, 14]               0\n",
      "     BatchNorm2d-286          [-1, 348, 14, 14]             696\n",
      "            ReLU-287          [-1, 348, 14, 14]               0\n",
      "          Conv2d-288           [-1, 48, 14, 14]          16,704\n",
      "     BatchNorm2d-289           [-1, 48, 14, 14]              96\n",
      "            ReLU-290           [-1, 48, 14, 14]               0\n",
      "          Conv2d-291           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-292          [-1, 360, 14, 14]               0\n",
      "     BatchNorm2d-293          [-1, 360, 14, 14]             720\n",
      "            ReLU-294          [-1, 360, 14, 14]               0\n",
      "          Conv2d-295           [-1, 48, 14, 14]          17,280\n",
      "     BatchNorm2d-296           [-1, 48, 14, 14]              96\n",
      "            ReLU-297           [-1, 48, 14, 14]               0\n",
      "          Conv2d-298           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-299          [-1, 372, 14, 14]               0\n",
      "     BatchNorm2d-300          [-1, 372, 14, 14]             744\n",
      "            ReLU-301          [-1, 372, 14, 14]               0\n",
      "          Conv2d-302           [-1, 48, 14, 14]          17,856\n",
      "     BatchNorm2d-303           [-1, 48, 14, 14]              96\n",
      "            ReLU-304           [-1, 48, 14, 14]               0\n",
      "          Conv2d-305           [-1, 12, 14, 14]           5,184\n",
      "      BottleNeck-306          [-1, 384, 14, 14]               0\n",
      "     BatchNorm2d-307          [-1, 384, 14, 14]             768\n",
      "            ReLU-308          [-1, 384, 14, 14]               0\n",
      "          Conv2d-309          [-1, 192, 14, 14]          73,728\n",
      "       AvgPool2d-310            [-1, 192, 7, 7]               0\n",
      "      Transition-311            [-1, 192, 7, 7]               0\n",
      "     BatchNorm2d-312            [-1, 192, 7, 7]             384\n",
      "            ReLU-313            [-1, 192, 7, 7]               0\n",
      "          Conv2d-314             [-1, 48, 7, 7]           9,216\n",
      "     BatchNorm2d-315             [-1, 48, 7, 7]              96\n",
      "            ReLU-316             [-1, 48, 7, 7]               0\n",
      "          Conv2d-317             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-318            [-1, 204, 7, 7]               0\n",
      "     BatchNorm2d-319            [-1, 204, 7, 7]             408\n",
      "            ReLU-320            [-1, 204, 7, 7]               0\n",
      "          Conv2d-321             [-1, 48, 7, 7]           9,792\n",
      "     BatchNorm2d-322             [-1, 48, 7, 7]              96\n",
      "            ReLU-323             [-1, 48, 7, 7]               0\n",
      "          Conv2d-324             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-325            [-1, 216, 7, 7]               0\n",
      "     BatchNorm2d-326            [-1, 216, 7, 7]             432\n",
      "            ReLU-327            [-1, 216, 7, 7]               0\n",
      "          Conv2d-328             [-1, 48, 7, 7]          10,368\n",
      "     BatchNorm2d-329             [-1, 48, 7, 7]              96\n",
      "            ReLU-330             [-1, 48, 7, 7]               0\n",
      "          Conv2d-331             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-332            [-1, 228, 7, 7]               0\n",
      "     BatchNorm2d-333            [-1, 228, 7, 7]             456\n",
      "            ReLU-334            [-1, 228, 7, 7]               0\n",
      "          Conv2d-335             [-1, 48, 7, 7]          10,944\n",
      "     BatchNorm2d-336             [-1, 48, 7, 7]              96\n",
      "            ReLU-337             [-1, 48, 7, 7]               0\n",
      "          Conv2d-338             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-339            [-1, 240, 7, 7]               0\n",
      "     BatchNorm2d-340            [-1, 240, 7, 7]             480\n",
      "            ReLU-341            [-1, 240, 7, 7]               0\n",
      "          Conv2d-342             [-1, 48, 7, 7]          11,520\n",
      "     BatchNorm2d-343             [-1, 48, 7, 7]              96\n",
      "            ReLU-344             [-1, 48, 7, 7]               0\n",
      "          Conv2d-345             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-346            [-1, 252, 7, 7]               0\n",
      "     BatchNorm2d-347            [-1, 252, 7, 7]             504\n",
      "            ReLU-348            [-1, 252, 7, 7]               0\n",
      "          Conv2d-349             [-1, 48, 7, 7]          12,096\n",
      "     BatchNorm2d-350             [-1, 48, 7, 7]              96\n",
      "            ReLU-351             [-1, 48, 7, 7]               0\n",
      "          Conv2d-352             [-1, 12, 7, 7]           5,184\n",
      "      BottleNeck-353            [-1, 264, 7, 7]               0\n",
      "     BatchNorm2d-354            [-1, 264, 7, 7]             528\n",
      "            ReLU-355            [-1, 264, 7, 7]               0\n",
      "AdaptiveAvgPool2d-356            [-1, 264, 1, 1]               0\n",
      "          Linear-357                   [-1, 10]           2,650\n",
      "================================================================\n",
      "Total params: 790,282\n",
      "Trainable params: 790,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 131.20\n",
      "Params size (MB): 3.01\n",
      "Estimated Total Size (MB): 134.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da2b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
